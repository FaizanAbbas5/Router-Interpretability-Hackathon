{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Judge 2 Gold Score Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a judge for evaluating the reason provided by the model when answering to general questions.\n",
    "The model will give the score as follows:\n",
    "- ⁠0 when the reason does not make sense and the answer is wrong.\n",
    "- 1 when the reason makes sense but does not lead to the expected answer, and the answer is wrong.\n",
    "- 2 when the reason makes sense and lead to the expected answer, but the answer is wrong.\n",
    "- 3 when the reason makes sense and lead to the expected answer, and the answer is correct and corresponding to the reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T17:23:12.991434Z",
     "start_time": "2025-05-28T17:23:12.985268Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "import logging\n",
    "import statistics\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "import openai\n",
    "from openai.types.chat import (\n",
    "    chat_completion,\n",
    "    chat_completion_message,\n",
    ")\n",
    "import sklearn.metrics\n",
    "\n",
    "from martian_apart_hack_sdk import exceptions, judge_specs, martian_client, utils\n",
    "from martian_apart_hack_sdk.models import judge_evaluation, llm_models, router_constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Credentials\n",
    "You must have a .env file with the following values set:\n",
    "\n",
    "1. `MARTIAN_API_URL` - withmartian.com/api\n",
    "1. `MARTIAN_API_KEY` - your personal API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T17:23:13.453321Z",
     "start_time": "2025-05-28T17:23:13.038954Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the config and make a client.\n",
    "config = utils.load_config()\n",
    "client = martian_client.MartianClient(\n",
    "    api_url=config.api_url,\n",
    "    api_key=config.api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrganizationBalance(credits=50.0)\n"
     ]
    }
   ],
   "source": [
    "# One quick thing we can do with the client is confirm we have credits.\n",
    "credit_balance = client.organization.get_credit_balance()\n",
    "print(credit_balance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Judging\n",
    "The prompt used for the judge model and the rubrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric = \"\"\"\n",
    "You are tasked with evaluating whether the answer of a model is correct to the question and whether the reason make sense. You will be doing this by reading the REASON part of the answer and evaluate it with the question to see if the reason make sense and if it actually leads to the answer, and check if the ANSWER part corresponds to the reason.\n",
    "The scoring is as follows:\n",
    "- ⁠0 when the reason does not make sense and the answer is wrong.\n",
    "- 1 when the reason is correct according to the context but has nothing to do with the question, and the answer is wrong.\n",
    "- 2 when the reason makes sense and lead to the expected answer, but the answer is wrong.\n",
    "- 3 when the reason makes sense and lead to the expected answer, and the answer is correct and corresponding to the reason.\n",
    "- ⁠Show the logic chain you use that lead to your judge.\n",
    "\"\"\"\n",
    "\n",
    "rubric_judge_spec = judge_specs.RubricJudgeSpec(\n",
    "\tmodel_type=\"rubric_judge\",\n",
    "\trubric=rubric,\n",
    "\tmodel=\"openai/openai/gpt-4o\",\n",
    "\tmin_score=0,\n",
    "\tmax_score=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the judge spec.\n",
    "def eval(question, answer):\n",
    "\tchat_request_text = question\n",
    "\tchat_response_text = answer\n",
    "\n",
    "\tcompletion_request = {\n",
    "\t\t\"model\": llm_models.GPT_4O_MINI,\n",
    "\t\t\"messages\": [{\"role\": \"user\", \"content\": chat_request_text}],\n",
    "\t}\n",
    "\n",
    "\tchat_completion_response = chat_completion.ChatCompletion(\n",
    "\t\tid=\"123\",\n",
    "\t\tchoices=[\n",
    "\t\t\tchat_completion.Choice(\n",
    "\t\t\t\tfinish_reason=\"stop\",\n",
    "\t\t\t\tindex=0,\n",
    "\t\t\t\tmessage=chat_completion_message.ChatCompletionMessage(\n",
    "\t\t\t\t\trole=\"assistant\",\n",
    "\t\t\t\t\tcontent=chat_response_text,\n",
    "\t\t\t\t),\n",
    "\t\t\t)\n",
    "\t\t],\n",
    "\t\tcreated=0,\n",
    "\t\tmodel=\"gpt-4o\",\n",
    "\t\tobject=\"chat.completion\",\n",
    "\t\tservice_tier=None,\n",
    "\t)\n",
    "\n",
    "\tevaluation_result = client.judges.evaluate_using_judge_spec(\n",
    "\t\trubric_judge_spec.to_dict(),\n",
    "\t\tcompletion_request=completion_request,\n",
    "\t\tcompletion_response=chat_completion_response,\n",
    "\t)\n",
    "\n",
    "\tprint(f\"Evaluation result: {evaluation_result}\")\n",
    "\treturn int(evaluation_result.score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gold Score Test\n",
    "Measuring the IAA (Inter-Annotator Agreement) of the judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = [\n",
    "    \"\"\"Original text: This compact study features a single, worn brown leather armchair facing a substantial oak desk. Three white ceramic mugs, filled with various pens and pencils, stand neatly in the desk's left corner, contrasting with the dark wood.\n",
    "    What is the colour of the leather armchair?\n",
    "    Answer in the language of the original text, answer in the following format:\n",
    "    ANSWER: your answer.\n",
    "    REASON: provide your reason.\"\"\",\n",
    "    \"\"\"Original text: This compact study features a single, worn brown leather armchair facing a substantial oak desk. Three white ceramic mugs, filled with various pens and pencils, stand neatly in the desk's left corner, contrasting with the dark wood.\n",
    "    How many white ceramic mugs are there?\n",
    "    Answer in the language of the original text, answer in the following format:\n",
    "    ANSWER: your answer.\n",
    "    REASON: provide your reason.\"\"\",\n",
    "    \"\"\"Original text: This compact study features a single, worn brown leather armchair facing a substantial oak desk. Three white ceramic mugs, filled with various pens and pencils, stand neatly in the desk's left corner, contrasting with the dark wood.\n",
    "    How many armchair are there in the study?\n",
    "    Answer in the language of the original text, answer in the following format:\n",
    "    ANSWER: your answer.\n",
    "    REASON: provide your reason.\"\"\"\n",
    "]\n",
    "example_answer = [\n",
    "    # expected score: 3\n",
    "    \"\"\"ANSWER: brown\n",
    "    REASON: We know the answer from this sentence in the original text: worn brown leather armchair facing a substantial oak desk\"\"\",\n",
    "    # expected score 2\n",
    "    \"\"\"ANSWER: four\n",
    "    REASON: The text mentioned that there are three white ceramic mugs filled with various pens and pencils\"\"\",\n",
    "    # expected score 0\n",
    "    \"\"\"ANSWER: the study shows armchairs are better for human bodies\n",
    "    REASON: The study claims that armchairs are better for human bodies in long-term uses\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create the client.\n",
    "openai_client = openai.OpenAI(\n",
    "    api_key=config.api_key,\n",
    "    base_url=config.api_url + \"/openai/v2\"\n",
    ")\n",
    "score = {\"0\":0, \"1\":0, \"2\":0, \"3\":0}\n",
    "for prompt, answer in zip(example_prompt, example_answer):\n",
    "    evaluation_model_1 = eval(prompt, answer)\n",
    "    \n",
    "    if evaluation_model_1 == 0:\n",
    "        score[\"0\"] = score[\"0\"] + 1\n",
    "    elif evaluation_model_1 == 1:\n",
    "        score[\"1\"] = score[\"1\"] + 1\n",
    "    elif evaluation_model_1 == 2:\n",
    "        score[\"2\"] = score[\"2\"] + 1\n",
    "    elif evaluation_model_1 == 3:\n",
    "        score[\"3\"] = score[\"3\"] + 1\n",
    "print(score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Martian SDK Quickstart Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T17:23:12.991434Z",
     "start_time": "2025-05-28T17:23:12.985268Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "import logging\n",
    "import statistics\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "import openai\n",
    "from openai.types.chat import (\n",
    "    chat_completion,\n",
    "    chat_completion_message,\n",
    ")\n",
    "import sklearn.metrics\n",
    "\n",
    "from martian_apart_hack_sdk import exceptions, judge_specs, martian_client, utils\n",
    "from martian_apart_hack_sdk.models import judge_evaluation, llm_models, router_constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Credentials\n",
    "You must have a .env file with the following values set:\n",
    "\n",
    "1. `MARTIAN_API_URL` - withmartian.com/api\n",
    "1. `MARTIAN_API_KEY` - your personal API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T17:23:13.453321Z",
     "start_time": "2025-05-28T17:23:13.038954Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the config and make a client.\n",
    "config = utils.load_config()\n",
    "client = martian_client.MartianClient(\n",
    "    api_url=config.api_url,\n",
    "    api_key=config.api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrganizationBalance(credits=50.0)\n"
     ]
    }
   ],
   "source": [
    "# One quick thing we can do with the client is confirm we have credits.\n",
    "credit_balance = client.organization.get_credit_balance()\n",
    "print(credit_balance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Martian Gateway\n",
    "\n",
    "You can use Martian as a gateway to access a number of different LLM providers.\n",
    "\n",
    "To do so, you start by making an OpenAI client with the base_url set to the Martian API URL + \"/openai/v2\".\n",
    "\n",
    "Then you can use the client as you would when working with OpenAI.\n",
    "\n",
    "The list of available models are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL:\n",
      "  - together/Qwen/Qwen2.5-72B-Instruct-Turbo\n",
      "  - together/meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo\n",
      "  - together/google/gemma-2-27b-it\n",
      "  - gemini/gemini/gemini-1.5-pro\n",
      "  - gemini/gemini/gemini-2.0-flash\n",
      "  - together/mistralai/Mistral-Small-24B-Instruct-2501\n",
      "  - openai/openai/gpt-4.1\n",
      "  - openai/openai/gpt-4.5-preview\n",
      "  - gemini/gemini/gemini-1.5-flash-8b-latest\n",
      "  - anthropic/anthropic/claude-3-opus-latest\n",
      "  - together/meta-llama/Llama-3.3-70B-Instruct-Turbo\n",
      "  - together/deepseek-ai/DeepSeek-V3\n",
      "  - together/nvidia/Llama-3.1-Nemotron-70B-Instruct-HF\n",
      "  - openai/openai/gpt-4o-mini\n",
      "  - gemini/gemini/gemini-1.5-flash-8b\n",
      "  - together/deepseek-ai/DeepSeek-R1\n",
      "  - gemini/gemini/gemini-1.5-flash-latest\n",
      "  - anthropic/anthropic/claude-3-5-haiku-latest\n",
      "  - gemini/gemini/gemini-1.5-flash\n",
      "  - openai/openai/gpt-4o\n",
      "  - anthropic/anthropic/claude-3-5-sonnet-latest\n",
      "  - openai/openai/gpt-4.1-nano\n",
      "  - gemini/gemini/gemini-1.5-pro-latest\n",
      "  - together/Qwen/Qwen2.5-Coder-32B-Instruct\n",
      "  - openai/openai/gpt-4.1-mini\n",
      "  - anthropic/anthropic/claude-3-7-sonnet-latest\n",
      "\n",
      "You could also pick any model from llm_models.ALL_MODELS\n"
     ]
    }
   ],
   "source": [
    "PROVIDERS = {\n",
    "    # \"OpenAI\": llm_models.OPENAI_MODELS,\n",
    "    # \"Anthropic\": llm_models.ANTHROPIC_MODELS,\n",
    "    # \"Together\": llm_models.TOGETHER_MODELS,\n",
    "    # \"Google Gemini\": llm_models.GEMINI_MODELS,\n",
    "    \"ALL\":llm_models.ALL_MODELS\n",
    "}\n",
    "\n",
    "for provider, models in PROVIDERS.items():\n",
    "    print(f'{provider}:')\n",
    "    for model in models:\n",
    "        print(f'  - {model}')\n",
    "    print()\n",
    "print(\"You could also pick any model from llm_models.ALL_MODELS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric = \"\"\"\n",
    "You are tasked with evaluating whether the answer of a model is correct to the question and whether the reason make sense. You will be doing this by reading the REASON part of the answer and evaluate it with the question to see if the reason make sense and if it actually leads to the answer, and check if the ANSWER part corresponds to the reason.\n",
    "The scoring is as follows:\n",
    "- ⁠0 when the reason does not make sense and the answer is wrong.\n",
    "- 1 when the reason is correct according to the context but has nothing to do with the question, and the answer is wrong.\n",
    "- 2 when the reason makes sense and lead to the expected answer, but the answer is wrong.\n",
    "- 3 when the reason makes sense and lead to the expected answer, and the answer is correct and corresponding to the reason.\n",
    "- ⁠Show the logic chain you use that lead to your judge.\n",
    "\"\"\"\n",
    "\n",
    "rubric_judge_spec = judge_specs.RubricJudgeSpec(\n",
    "\tmodel_type=\"rubric_judge\",\n",
    "\trubric=rubric,\n",
    "\tmodel=\"openai/openai/gpt-4o\",\n",
    "\tmin_score=0,\n",
    "\tmax_score=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the judge spec.\n",
    "def eval(question, answer):\n",
    "\tchat_request_text = question\n",
    "\tchat_response_text = answer\n",
    "\n",
    "\tcompletion_request = {\n",
    "\t\t\"model\": llm_models.GPT_4O_MINI,\n",
    "\t\t\"messages\": [{\"role\": \"user\", \"content\": chat_request_text}],\n",
    "\t}\n",
    "\n",
    "\tchat_completion_response = chat_completion.ChatCompletion(\n",
    "\t\tid=\"123\",\n",
    "\t\tchoices=[\n",
    "\t\t\tchat_completion.Choice(\n",
    "\t\t\t\tfinish_reason=\"stop\",\n",
    "\t\t\t\tindex=0,\n",
    "\t\t\t\tmessage=chat_completion_message.ChatCompletionMessage(\n",
    "\t\t\t\t\trole=\"assistant\",\n",
    "\t\t\t\t\tcontent=chat_response_text,\n",
    "\t\t\t\t),\n",
    "\t\t\t)\n",
    "\t\t],\n",
    "\t\tcreated=0,\n",
    "\t\tmodel=\"gpt-4o\",\n",
    "\t\tobject=\"chat.completion\",\n",
    "\t\tservice_tier=None,\n",
    "\t)\n",
    "\n",
    "\tevaluation_result = client.judges.evaluate_using_judge_spec(\n",
    "\t\trubric_judge_spec.to_dict(),\n",
    "\t\tcompletion_request=completion_request,\n",
    "\t\tcompletion_response=chat_completion_response,\n",
    "\t)\n",
    "\n",
    "\t# print(f\"Evaluation result: {evaluation_result}\")\n",
    "\treturn int(evaluation_result.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create the client.\n",
    "openai_client = openai.OpenAI(\n",
    "    api_key=config.api_key,\n",
    "    base_url=config.api_url + \"/openai/v2\"\n",
    ")\n",
    "\n",
    "df_en = pd.read_parquet(\"hf://datasets/google/xquad/xquad.en/validation-00000-of-00001.parquet\")\n",
    "df_en_5 = df_en.head(100)\n",
    "results_en = []\n",
    "score_gemini_en = {\"0\":0, \"1\":0, \"2\":0, \"3\":0}\n",
    "score_gpt_en = {\"0\":0, \"1\":0, \"2\":0, \"3\":0}\n",
    "score_llama_en = {\"0\":0, \"1\":0, \"2\":0, \"3\":0}\n",
    "\n",
    "for idx, lines in df_en_5.iterrows():\n",
    "\tresults_en.append([lines[\"context\"],lines[\"question\"],lines[\"answers\"]])\n",
    "\n",
    "for context, question, answers in results_en:\n",
    "    prompt_en = f\"Original text:{context}\\n{question}\\nAnswer in the language of the original text, answer in the following format:\\nANSWER: your answer.\\nREASON: provide your reason.\"\n",
    "    gemini_8b_response_en = openai_client.chat.completions.create(\n",
    "        model=\"gemini/gemini/gemini-1.5-flash-8b\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt_en}],\n",
    "    )\n",
    "    gpt_41nano_response_en = openai_client.chat.completions.create(\n",
    "        model=\"openai/openai/gpt-4.1-nano\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt_en}],\n",
    "    )\n",
    "    llama31405B_response_en = openai_client.chat.completions.create(\n",
    "        model=\"together/meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt_en}],\n",
    "    )\n",
    "    # print(prompt_en)\n",
    "    # print(\"THE EXPECTED ANSWER is\",answers['text'][0],\"\\n\")\n",
    "    # print(\"\\n 1.\\n\")\n",
    "    # print(\"Mistral Samll says:\", mistral_small_response_en.choices[0].message.content)\n",
    "    # print(\"\\n 2.\\n\")\n",
    "    # print(\"DeepSeek-V3 says:\", DeepSeekV3_response_en.choices[0].message.content)\n",
    "    # print(\"\\n 3.\\n\")\n",
    "    # print(\"Llama-3.1-405B-Instruct-Turbo says:\", llama31405B_response_en.choices[0].message.content)\n",
    "    # print(\"\\n xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \\n\")\n",
    "\n",
    "    evaluation_model_1_en = eval(prompt_en, gemini_8b_response_en.choices[0].message.content)\n",
    "    evaluation_model_2_en = eval(prompt_en, gpt_41nano_response_en.choices[0].message.content)\n",
    "    evaluation_model_3_en = eval(prompt_en, llama31405B_response_en.choices[0].message.content)\n",
    "    \n",
    "    if evaluation_model_1_en == 0:\n",
    "        score_gemini_en[\"0\"] = score_gemini_en[\"0\"] + 1\n",
    "    elif evaluation_model_1_en == 1:\n",
    "        score_gemini_en[\"1\"] = score_gemini_en[\"1\"] + 1\n",
    "    elif evaluation_model_1_en == 2:\n",
    "        score_gemini_en[\"2\"] = score_gemini_en[\"2\"] + 1\n",
    "    elif evaluation_model_1_en == 3:\n",
    "        score_gemini_en[\"3\"] = score_gemini_en[\"3\"] + 1\n",
    "\n",
    "    if evaluation_model_2_en == 0:\n",
    "        score_gpt_en[\"0\"] = score_gpt_en[\"0\"] + 1\n",
    "    elif evaluation_model_2_en == 1:\n",
    "        score_gpt_en[\"1\"] = score_gpt_en[\"1\"] + 1\n",
    "    elif evaluation_model_2_en == 2:\n",
    "        score_gpt_en[\"2\"] = score_gpt_en[\"2\"] + 1\n",
    "    elif evaluation_model_2_en == 3:\n",
    "        score_gpt_en[\"3\"] = score_gpt_en[\"3\"] + 1\n",
    "\n",
    "    if evaluation_model_3_en == 0:\n",
    "        score_llama_en[\"0\"] = score_llama_en[\"0\"] + 1\n",
    "    elif evaluation_model_3_en == 1:\n",
    "        score_llama_en[\"1\"] = score_llama_en[\"1\"] + 1\n",
    "    elif evaluation_model_3_en == 2:\n",
    "        score_llama_en[\"2\"] = score_llama_en[\"2\"] + 1\n",
    "    elif evaluation_model_3_en == 3:\n",
    "        score_llama_en[\"3\"] = score_llama_en[\"3\"] + 1\n",
    "    \n",
    "\n",
    "df_zh = pd.read_parquet(\"hf://datasets/google/xquad/xquad.zh/validation-00000-of-00001.parquet\")\n",
    "df_zh_5 = df_zh.head(100)\n",
    "results_zh = []\n",
    "score_gemini_zh = {\"0\":0, \"1\":0, \"2\":0, \"3\":0}\n",
    "score_gpt_zh = {\"0\":0, \"1\":0, \"2\":0, \"3\":0}\n",
    "score_llama_zh = {\"0\":0, \"1\":0, \"2\":0, \"3\":0}\n",
    "\n",
    "for idx, lines in df_zh_5.iterrows():\n",
    "\tresults_zh.append([lines[\"context\"],lines[\"question\"],lines[\"answers\"]])\n",
    "\n",
    "for context, question, answers in results_zh:\n",
    "    prompt_zh = f\"Original text:{context}\\n{question}\\nAnswer in the language of the original text, answer in the following format:\\nANSWER: your answer.\\nREASON: provide your reason.\"\n",
    "    gemini_8b_response_zh = openai_client.chat.completions.create(\n",
    "        model=\"gemini/gemini/gemini-1.5-flash-8b\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt_zh}],\n",
    "    )\n",
    "    gpt_41nano_response_zh = openai_client.chat.completions.create(\n",
    "        model=\"openai/openai/gpt-4.1-nano\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt_zh}],\n",
    "    )\n",
    "    llama31405B_response_zh = openai_client.chat.completions.create(\n",
    "        model=\"together/meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt_zh}],\n",
    "    )\n",
    "    # print(prompt_zh)\n",
    "    # print(\"THE EXPECTED ANSWER is\",answers['text'][0],\"\\n\")\n",
    "    # print(\"\\n 1.\\n\")\n",
    "    # print(\"gemini-1.5-flash-8b-latest says:\", mistral_small_response_zh.choices[0].message.content)\n",
    "    # print(\"\\n 2.\\n\")\n",
    "    # print(\"DeepSeek-V3 says:\", DeepSeekV3_response_zh.choices[0].message.content)\n",
    "    # print(\"\\n 3.\\n\")\n",
    "    # print(\"Llama-3.1-405B-Instruct-Turbo says:\", llama31405B_response_zh.choices[0].message.content)\n",
    "    # print(\"\\n xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \\n\")\n",
    "\n",
    "    evaluation_model_1_zh = eval(prompt_zh, gemini_8b_response_zh.choices[0].message.content)\n",
    "    evaluation_model_2_zh = eval(prompt_zh, gpt_41nano_response_zh.choices[0].message.content)\n",
    "    evaluation_model_3_zh = eval(prompt_zh, llama31405B_response_zh.choices[0].message.content)\n",
    "    \n",
    "    if evaluation_model_1_zh == 0:\n",
    "        score_gemini_zh[\"0\"] = score_gemini_zh[\"0\"] + 1\n",
    "    elif evaluation_model_1_zh == 1:\n",
    "        score_gemini_zh[\"1\"] = score_gemini_zh[\"1\"] + 1\n",
    "    elif evaluation_model_1_zh == 2:\n",
    "        score_gemini_zh[\"2\"] = score_gemini_zh[\"2\"] + 1\n",
    "    elif evaluation_model_1_zh == 3:\n",
    "        score_gemini_zh[\"3\"] = score_gemini_zh[\"3\"] + 1\n",
    "\n",
    "    if evaluation_model_2_zh == 0:\n",
    "        score_gpt_zh[\"0\"] = score_gpt_zh[\"0\"] + 1\n",
    "    elif evaluation_model_2_zh == 1:\n",
    "        score_gpt_zh[\"1\"] = score_gpt_zh[\"1\"] + 1\n",
    "    elif evaluation_model_2_zh == 2:\n",
    "        score_gpt_zh[\"2\"] = score_gpt_zh[\"2\"] + 1\n",
    "    elif evaluation_model_2_zh == 3:\n",
    "        score_gpt_zh[\"3\"] = score_gpt_zh[\"3\"] + 1\n",
    "\n",
    "    if evaluation_model_3_zh == 0:\n",
    "        score_llama_zh[\"0\"] = score_llama_zh[\"0\"] + 1\n",
    "    elif evaluation_model_3_zh == 1:\n",
    "        score_llama_zh[\"1\"] = score_llama_zh[\"1\"] + 1\n",
    "    elif evaluation_model_3_zh == 2:\n",
    "        score_llama_zh[\"2\"] = score_llama_zh[\"2\"] + 1\n",
    "    elif evaluation_model_3_zh == 3:\n",
    "        score_llama_zh[\"3\"] = score_llama_zh[\"3\"] + 1\n",
    "\n",
    "print(\"EN\")\n",
    "print(f\"Gemini: {score_gemini_en}\")\n",
    "print(f\"GPT40mini: {score_gpt_en}\")\n",
    "print(f\"llama: {score_llama_en}\")\n",
    "print(\"ZH\")\n",
    "print(f\"Gemini: {score_gemini_zh}\")\n",
    "print(f\"GPT40mini: {score_gpt_zh}\")\n",
    "print(f\"llama: {score_llama_zh}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
